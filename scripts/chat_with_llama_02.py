from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import spacy

# â€” â€” Setup â€” â€”
collection_name = "local_files"
query = input("ğŸ§  Ask your question: ").strip()

# â€” â€” Load models â€” â€”
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
llm = Llama(
    model_path="/home/ali/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-Chat-GGUF/"
               "snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/"
               "llama-2-7b-chat.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
)

nlp = spacy.load("en_core_web_sm")

# â€” â€” Retrieve topâ€3 docs â€” â€”
client = QdrantClient(host="localhost", port=6333)
vector = embedder.encode(query).tolist()
hits = client.search(collection_name=collection_name, query_vector=vector, limit=3)

# â€” â€” Build sourceâ€annotated context â€” â€”
context_blocks = []
for i, hit in enumerate(hits, 1):
    src = hit.payload.get("file_path", "Unknown")
    text = hit.payload.get("content", "")[:500]  # truncate
    context_blocks.append(f"[DOC{i}: {src}]\n{text}")

context = "\n\n".join(context_blocks)

# â€” â€” Extract NER & roles (optional) â€” â€”
# â€¦ your spacy & â€œSeller/Buyerâ€ regex code â€¦

# â€” â€” Final Prompt â€” â€”
prompt = f"""[INST] <<SYS>>
You are a precise documentâ€analysis assistant.
Use the three provided documents (DOC1, DOC2, DOC3) to answer and also tell me which DOC you used.
<</SYS>>

Context:
{context}

Question: {query}

Please answer in this format, quoting the DOC you relied on most:
Answer: <your answer>
Source: <DOC#>
[/INST]"""

# â€” â€” Ask LLaMA â€” â€”
resp = llm(prompt, max_tokens=200, stop=["[/INST]"])
print(resp["choices"][0]["text"].strip())

del llm

