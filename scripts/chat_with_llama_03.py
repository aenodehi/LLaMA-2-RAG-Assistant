#!/usr/bin/env python3
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import spacy

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COLLECTION = "local_files"
MODEL_PATH = (
    "/home/ali/.cache/huggingface/hub/models--TheBloke--"
    "Llama-2-7B-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/"
    "llama-2-7b-chat.Q4_K_M.gguf"
)
MAX_CHARS = 1000  # keep each chunk small to avoid overflow

# â”€â”€â”€ INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query = input("ğŸ§  Ask your question: ").strip()
if not query:
    print("âš ï¸ Empty query. Exiting.")
    exit(1)

# â”€â”€â”€ LOAD MODELS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=4)
nlp = spacy.load("en_core_web_sm")

# â”€â”€â”€ VECTOR SEARCH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = QdrantClient(host="localhost", port=6333)
vec = embedder.encode(query).tolist()
hits = client.search(
    collection_name=COLLECTION,
    query_vector=vec,
    limit=1,
    with_payload=True
)

# â”€â”€â”€ BUILD CONTEXT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
blocks = []
for hit in hits:
    src = hit.payload.get("file_path", "unknown")
    text = hit.payload.get("content", "")
    snippet = text[:MAX_CHARS].replace("\n", " ")
    blocks.append(f"[Source: {src}]\n{snippet}")
context = "\n\n".join(blocks)

# â”€â”€â”€ NER & ROLE EXTRACTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
doc = nlp(context)
entities = [
    (ent.text, ent.label_)
    for ent in doc.ents
    if ent.label_ in {"ORG", "PERSON", "GPE", "DATE"}
]

roles = {}
for line in context.splitlines():
    low = line.lower()
    if "seller" in low and "Seller" not in roles:
        roles["Seller"] = line.strip()
    if "buyer" in low and "Buyer" not in roles:
        roles["Buyer"] = line.strip()
    if "law" in low and "Law" not in roles:
        roles["Law"] = line.strip()
    if "arbitration" in low and "Arbitration" not in roles:
        roles["Arbitration"] = line.strip()

# â”€â”€â”€ STRUCTURED PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
structured = "ğŸ“Œ Roles:\n" + "\n".join(f"{k}: {v}" for k, v in roles.items())
structured += "\n\nğŸ” Entities:\n" + "\n".join(f"{lbl}: {txt}" for txt, lbl in entities)

prompt = f"""[INST] <<SYS>>
You are a documentâ€analysis assistant. Use the roles & entities to answer precisely.
<</SYS>>

{structured}

ğŸ“„ Raw Context:
{context}

â“ Question: {query}
Answer: [/INST]"""

# â”€â”€â”€ CALL LLaMA & CLEANUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    out = llm(prompt, max_tokens=300, stop=["</s>"])
    print("\nğŸ¦™ Response:\n" + out["choices"][0]["text"].strip())
finally:
    del llm

